{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b9950b-af18-48f4-a9a6-74cc393bb1e3",
   "metadata": {},
   "source": [
    "## Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5535be8-3536-410c-97fa-e49ac230e578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyathena in /opt/conda/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /opt/conda/lib/python3.10/site-packages (from pyathena) (1.34.84)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /opt/conda/lib/python3.10/site-packages (from pyathena) (1.34.84)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from pyathena) (2024.3.1)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from pyathena) (8.2.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.26.4->pyathena) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.26.4->pyathena) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.29.4->pyathena) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.29.4->pyathena) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->pyathena) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyathena\n",
    "!pip install -U sagemaker -q\n",
    "!pip install boto3 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418143a4-de6d-41c8-8186-91b5cdc2e61b",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dec9592-264c-4b29-84e0-7f9aff98b115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.session import Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0898272-221a-4805-af39-123f7b2f0fd2",
   "metadata": {},
   "source": [
    "## Set Up Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826e09a4-4d18-41e0-9918-b04decdff3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = \"ProductRecommendationSystem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "813dae97-2711-49e9-8432-fa67332b5f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "model_package_group_name = f\"ProductRecommendationModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af8f44fa-e92d-4066-bde0-20ce4a3197fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define S3 bucket and data paths\n",
    "bucket = 'electronics-dataset'\n",
    "prefix = 'data'\n",
    "s3_train_data = f\"s3://{bucket}/{prefix}/train/\"\n",
    "s3_val_data = f\"s3://{bucket}/{prefix}/validation/\"\n",
    "s3_test_data = f\"s3://{bucket}/{prefix}/test/\"\n",
    "s3_prod_data = f\"s3://{bucket}/{prefix}/production/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734c633-8f6c-446e-9145-93ac61600e97",
   "metadata": {},
   "source": [
    "## Define Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "954e8517-ab01-4fc8-8b8c-3415cb7c9ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "train_data = ParameterString(\n",
    "    name=\"TrainData\",\n",
    "    default_value=f\"s3://{bucket}/{prefix}/train/train_data.csv\",\n",
    ")\n",
    "val_data = ParameterString(\n",
    "    name=\"ValData\",\n",
    "    default_value=f\"s3://{bucket}/{prefix}/validation/validation_data.csv\",\n",
    ")\n",
    "test_data = ParameterString(\n",
    "    name=\"TestData\",\n",
    "    default_value=f\"s3://{bucket}/{prefix}/test/test_data_with_outcome.csv\",\n",
    ")\n",
    "\n",
    "acc_threshold = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc582001-1025-4b0d-9981-844561c571dc",
   "metadata": {},
   "source": [
    "## Define Processing Step for Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78259e0-a5a4-4d16-85b4-f85c2c4e31c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the Code directory if it doesn't exist\n",
    "os.makedirs(\"Code\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05b93f4b-4f59-4a3a-9401-4740c62a743f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    train = pd.read_csv(f\"{base_dir}/train1/train_data.csv\", header=None)\n",
    "    test = pd.read_csv(f\"{base_dir}/test1/test_data_with_outcome.csv\", header=None)\n",
    "    validation = pd.read_csv(f\"{base_dir}/val1/validation_data.csv\", header=None)\n",
    "    \n",
    "    test = test[:round(len(test) * 0.8)]\n",
    "    prod = test[round(len(test) * 0.8):].drop(test.columns[0], axis=1)\n",
    "\n",
    "    train.to_csv(f\"{base_dir}/train2/train.csv\", header=False, index=False)\n",
    "    validation.to_csv(f\"{base_dir}/validation2/validation.csv\", header=False, index=False)\n",
    "    test.to_csv(f\"{base_dir}/test2/test.csv\", header=False, index=False)\n",
    "    prod.to_csv(f\"{base_dir}/prod2/prod.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5ef1b13-a8bb-49ab-ac67-7875bc5bb93c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=train_data, destination=\"/opt/ml/processing/train1\"),\n",
    "        ProcessingInput(source=val_data, destination=\"/opt/ml/processing/val1\"),\n",
    "        ProcessingInput(source=test_data, destination=\"/opt/ml/processing/test1\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"proc_train_out\", source=\"/opt/ml/processing/train2\"),\n",
    "        ProcessingOutput(output_name=\"proc_validation_out\", source=\"/opt/ml/processing/validation2\"),\n",
    "        ProcessingOutput(output_name=\"proc_test_out\", source=\"/opt/ml/processing/test2\"),\n",
    "        ProcessingOutput(output_name=\"proc_prod_out\", source=\"/opt/ml/processing/prod2\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"Process\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777ea5c-a988-4202-a201-05af1b959590",
   "metadata": {},
   "source": [
    "### Training Step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81a26466-7714-490a-9a4e-090927169030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "model_path = f\"s3://{bucket}/{prefix}/model\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    ")\n",
    "\n",
    "train_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"proc_train_out\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"proc_validation_out\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    step_args=train_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17596ad-529e-4d30-8eb1-3dba49e376f8",
   "metadata": {},
   "source": [
    "### Model Evaluation Step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b7831f3-082d-4d84-b1e1-b81e5d70d013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import tarfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test2/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "\n",
    "    report_dict = {\n",
    "        \"classification_metrics\": {\n",
    "            \"accuracy\": acc,\n",
    "        },\n",
    "    }\n",
    "    print(acc)\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cb22c-9435-4b98-bcf8-f548025658c5",
   "metadata": {},
   "source": [
    "###  Evaluation Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7221594-f9df-4ea7-8430-f678301dc4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"proc_test_out\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test2\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"Evaluate\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a8743-746f-4d2d-bc02-d4adc3635a08",
   "metadata": {},
   "source": [
    "### Model Registration Step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2b6bf9f-d643-4dd7-85cb-a153dd2d3090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=f\"{step_eval.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']}/evaluation.json\",\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "step_register = ModelStep(name=\"RegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b15174-2d5b-4268-bea7-a11f3f257bfb",
   "metadata": {},
   "source": [
    "###  Conditional Step for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c3b4d2b-4917-4b0f-8d0b-2a3250ad56c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"AccuracyFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to Accuracy <\", acc_threshold]),\n",
    ")\n",
    "\n",
    "cond_lte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.accuracy\",\n",
    "    ),\n",
    "    right=acc_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e665f-1b14-4835-ba3a-dedfbbb3d7b5",
   "metadata": {},
   "source": [
    "### Define Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a875d727-d98d-44a4-8654-c1b4ddf05ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:676076160400:pipeline/ProductRecommendationPipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'e5e09c68-50a8-44bf-8d20-e278be11b7e0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e5e09c68-50a8-44bf-8d20-e278be11b7e0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '97',\n",
       "   'date': 'Mon, 01 Jul 2024 17:35:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"ProductRecommendationPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        test_data,\n",
    "        acc_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8037355-6788-46e3-9f8a-1d814ee0b0bf",
   "metadata": {},
   "source": [
    "### Start Pipeline Execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7769c1c-be2b-42ad-885a-55904703d72d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "WaiterError",
     "evalue": "Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaiterError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m execution\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Wait for the pipeline execution to complete\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mexecution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# List the steps of the pipeline execution and their statuses\u001b[39;00m\n\u001b[1;32m      8\u001b[0m steps \u001b[38;5;241m=\u001b[39m execution\u001b[38;5;241m.\u001b[39mlist_steps()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline.py:934\u001b[0m, in \u001b[0;36m_PipelineExecution.wait\u001b[0;34m(self, delay, max_attempts)\u001b[0m\n\u001b[1;32m    905\u001b[0m model \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mwaiter\u001b[38;5;241m.\u001b[39mWaiterModel(\n\u001b[1;32m    906\u001b[0m     {\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     }\n\u001b[1;32m    930\u001b[0m )\n\u001b[1;32m    931\u001b[0m waiter \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mwaiter\u001b[38;5;241m.\u001b[39mcreate_waiter_with_client(\n\u001b[1;32m    932\u001b[0m     waiter_id, model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_client\n\u001b[1;32m    933\u001b[0m )\n\u001b[0;32m--> 934\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPipelineExecutionArn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/waiter.py:55\u001b[0m, in \u001b[0;36mcreate_waiter_with_client.<locals>.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mWaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/waiter.py:375\u001b[0m, in \u001b[0;36mWaiter.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailure\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    372\u001b[0m     reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWaiter encountered a terminal failure state: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    373\u001b[0m         acceptor\u001b[38;5;241m.\u001b[39mexplanation\n\u001b[1;32m    374\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaiterError(\n\u001b[1;32m    376\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    377\u001b[0m         reason\u001b[38;5;241m=\u001b[39mreason,\n\u001b[1;32m    378\u001b[0m         last_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_attempts \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_attempts:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_matched_acceptor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mWaiterError\u001b[0m: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\""
     ]
    }
   ],
   "source": [
    "execution = pipeline.start()\n",
    "execution.describe()\n",
    "\n",
    "# Wait for the pipeline execution to complete\n",
    "execution.wait()\n",
    "\n",
    "# List the steps of the pipeline execution and their statuses\n",
    "steps = execution.list_steps()\n",
    "for step in steps:\n",
    "    print(f\"Step: {step['StepName']}, Status: {step['StepStatus']}, Start Time: {step['StartTime']}, End Time: {step['EndTime']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2a76e-83d6-40cf-9fb7-ef09516d41e1",
   "metadata": {},
   "source": [
    "### Start the Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72cc6495-27a9-4adf-b7c9-9afe13e0db78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Examine the resulting model evaluation after the pipeline completes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m----> 4\u001b[0m evaluation_json \u001b[38;5;241m=\u001b[39m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3Downloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstep_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessingOutputConfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS3Output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS3Uri\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/evaluation.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m pprint(json\u001b[38;5;241m.\u001b[39mloads(evaluation_json))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/s3.py:180\u001b[0m, in \u001b[0;36mS3Downloader.read_file\u001b[0;34m(s3_uri, sagemaker_session)\u001b[0m\n\u001b[1;32m    176\u001b[0m sagemaker_session \u001b[38;5;241m=\u001b[39m sagemaker_session \u001b[38;5;129;01mor\u001b[39;00m Session()\n\u001b[1;32m    178\u001b[0m bucket, object_key \u001b[38;5;241m=\u001b[39m parse_s3_url(url\u001b[38;5;241m=\u001b[39ms3_uri)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_s3_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:558\u001b[0m, in \u001b[0;36mSession.read_s3_file\u001b[0;34m(self, bucket, key_prefix)\u001b[0m\n\u001b[1;32m    555\u001b[0m     s3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3_client\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Explicitly passing a None kms_key to boto3 throws a validation error.\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m s3_object \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s3_object[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "# Examine the resulting model evaluation after the pipeline completes\n",
    "from pprint import pprint\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    f\"{step_eval.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']}/evaluation.json\"\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1f998-f56c-450e-9bb4-7139076df969",
   "metadata": {},
   "source": [
    "### Define a Register Model Step to Create a Model Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdbaeabd-cb44-4baf-8320-38097ab99d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded code to s3://sagemaker-us-east-1-676076160400/electronics-eval-2024-07-01-15-09-11-221/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-676076160400/electronics-eval-2024-07-01-15-09-11-221/source/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = pytorch_model.register(\n",
    "    content_types=[\"image/jpeg\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    "    domain=\"COMPUTER_VISION\"\n",
    ")\n",
    "\n",
    "step_register = ModelStep(name=\"ElectronicsRegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8d246-e93c-4959-8aee-f8f2b40503d7",
   "metadata": {},
   "source": [
    "### Fail Step to Terminate the Pipeline Execution and Mark it as Failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ca2793-2593-418c-b71b-2e10d614d91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_fail = FailStep(\n",
    "    name=\"ElectronicsMAPFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to mAP <\", map_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4467dd6-e256-4dff-8558-b6d8bf63036b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"detection_metrics.mAP50.value\",\n",
    "    ),\n",
    "    right=map_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"ElectronicsMAPCond\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b083a-dc60-4c64-abc7-8b3733d65d1c",
   "metadata": {},
   "source": [
    "### Define a Pipeline of Parameters, Steps, and Conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d919db3c-cab7-4356-9434-20805443b3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded code to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/1e4b7d6dc9f60d312a49dca13f45c47f/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/54f0ef6bee583ff9186b762aaf572190/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.processing:Uploaded code to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/049bf5e9d232ff7d9a27fce88e210f56/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.processing:Uploaded code to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/1e4b7d6dc9f60d312a49dca13f45c47f/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/54f0ef6bee583ff9186b762aaf572190/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Version\": \"2020-12-01\",\n",
      "  \"Metadata\": {},\n",
      "  \"Parameters\": [\n",
      "    {\n",
      "      \"Name\": \"ProcessingInstanceCount\",\n",
      "      \"Type\": \"Integer\",\n",
      "      \"DefaultValue\": 1\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"TrainingInstanceType\",\n",
      "      \"Type\": \"String\",\n",
      "      \"DefaultValue\": \"ml.m5.xlarge\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"ModelApprovalStatus\",\n",
      "      \"Type\": \"String\",\n",
      "      \"DefaultValue\": \"PendingManualApproval\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"mAPThreshold\",\n",
      "      \"Type\": \"Float\",\n",
      "      \"DefaultValue\": 0.001\n",
      "    }\n",
      "  ],\n",
      "  \"PipelineExperimentConfig\": {\n",
      "    \"ExperimentName\": {\n",
      "      \"Get\": \"Execution.PipelineName\"\n",
      "    },\n",
      "    \"TrialName\": {\n",
      "      \"Get\": \"Execution.PipelineExecutionId\"\n",
      "    }\n",
      "  },\n",
      "  \"Steps\": [\n",
      "    {\n",
      "      \"Name\": \"ElectronicsProcess\",\n",
      "      \"Type\": \"Processing\",\n",
      "      \"Arguments\": {\n",
      "        \"ProcessingResources\": {\n",
      "          \"ClusterConfig\": {\n",
      "            \"InstanceType\": \"ml.m5.xlarge\",\n",
      "            \"InstanceCount\": {\n",
      "              \"Get\": \"Parameters.ProcessingInstanceCount\"\n",
      "            },\n",
      "            \"VolumeSizeInGB\": 30\n",
      "          }\n",
      "        },\n",
      "        \"AppSpecification\": {\n",
      "          \"ImageUri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.1.0-cpu-py310\",\n",
      "          \"ContainerArguments\": [\n",
      "            \"--s3-images-source\",\n",
      "            \"s3://sagemaker-us-east-1-676076160400/electronics/data/images/\",\n",
      "            \"--s3-labels-source\",\n",
      "            \"s3://sagemaker-us-east-1-676076160400/electronics/data/labels/\",\n",
      "            \"--s3-split-dest\",\n",
      "            \"s3://sagemaker-us-east-1-676076160400/electronics/data/split_cicd/\"\n",
      "          ],\n",
      "          \"ContainerEntrypoint\": [\n",
      "            \"/bin/bash\",\n",
      "            \"/opt/ml/processing/input/entrypoint/runproc.sh\"\n",
      "          ]\n",
      "        },\n",
      "        \"RoleArn\": \"arn:aws:iam::676076160400:role/LabRole\",\n",
      "        \"ProcessingInputs\": [\n",
      "          {\n",
      "            \"InputName\": \"input-1\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": {\n",
      "                \"Get\": \"Parameters.InputData\"\n",
      "              },\n",
      "              \"LocalPath\": \"/opt/ml/processing/input\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"InputName\": \"code\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/1e4b7d6dc9f60d312a49dca13f45c47f/sourcedir.tar.gz\",\n",
      "              \"LocalPath\": \"/opt/ml/processing/input/code/\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"InputName\": \"entrypoint\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/54f0ef6bee583ff9186b762aaf572190/runproc.sh\",\n",
      "              \"LocalPath\": \"/opt/ml/processing/input/entrypoint\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"ElectronicsTrain\",\n",
      "      \"Type\": \"Training\",\n",
      "      \"Arguments\": {\n",
      "        \"AlgorithmSpecification\": {\n",
      "          \"TrainingInputMode\": \"File\",\n",
      "          \"TrainingImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.1.0-cpu-py310\",\n",
      "          \"EnableSageMakerMetricsTimeSeries\": true\n",
      "        },\n",
      "        \"OutputDataConfig\": {\n",
      "          \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/electronics/data/output/\"\n",
      "        },\n",
      "        \"StoppingCondition\": {\n",
      "          \"MaxRuntimeInSeconds\": 86400\n",
      "        },\n",
      "        \"ResourceConfig\": {\n",
      "          \"VolumeSizeInGB\": 30,\n",
      "          \"InstanceCount\": 1,\n",
      "          \"InstanceType\": {\n",
      "            \"Get\": \"Parameters.TrainingInstanceType\"\n",
      "          }\n",
      "        },\n",
      "        \"RoleArn\": \"arn:aws:iam::676076160400:role/LabRole\",\n",
      "        \"InputDataConfig\": [\n",
      "          {\n",
      "            \"DataSource\": {\n",
      "              \"S3DataSource\": {\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/electronics/data/split_cicd\",\n",
      "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "              }\n",
      "            },\n",
      "            \"ChannelName\": \"training\"\n",
      "          }\n",
      "        ],\n",
      "        \"HyperParameters\": {\n",
      "          \"data\": \"\\\"data.yaml\\\"\",\n",
      "          \"epochs\": \"3\",\n",
      "          \"batch\": \"32\",\n",
      "          \"yolo_model\": \"\\\"yolov8n.pt\\\"\",\n",
      "          \"saved_model_weights\": \"\\\"model.pt\\\"\",\n",
      "          \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/64b5d2a24c5da7a10ceacc6050178873/sourcedir.tar.gz\\\"\",\n",
      "          \"sagemaker_program\": \"\\\"train.py\\\"\",\n",
      "          \"sagemaker_container_log_level\": \"20\",\n",
      "          \"sagemaker_region\": \"\\\"us-east-1\\\"\"\n",
      "        },\n",
      "        \"DebugHookConfig\": {\n",
      "          \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/electronics/data/output/\",\n",
      "          \"CollectionConfigurations\": []\n",
      "        },\n",
      "        \"ProfilerConfig\": {\n",
      "          \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/electronics/data/output/\",\n",
      "          \"DisableProfiler\": false\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"ElectronicsEval\",\n",
      "      \"Type\": \"Processing\",\n",
      "      \"Arguments\": {\n",
      "        \"ProcessingResources\": {\n",
      "          \"ClusterConfig\": {\n",
      "            \"InstanceType\": \"ml.m5.xlarge\",\n",
      "            \"InstanceCount\": {\n",
      "              \"Get\": \"Parameters.ProcessingInstanceCount\"\n",
      "            },\n",
      "            \"VolumeSizeInGB\": 30\n",
      "          }\n",
      "        },\n",
      "        \"AppSpecification\": {\n",
      "          \"ImageUri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.1.0-cpu-py310\",\n",
      "          \"ContainerEntrypoint\": [\n",
      "            \"/bin/bash\",\n",
      "            \"/opt/ml/processing/input/entrypoint/runproc.sh\"\n",
      "          ]\n",
      "        },\n",
      "        \"RoleArn\": \"arn:aws:iam::676076160400:role/LabRole\",\n",
      "        \"ProcessingInputs\": [\n",
      "          {\n",
      "            \"InputName\": \"input-1\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": {\n",
      "                \"Get\": \"Steps.ElectronicsTrain.ModelArtifacts.S3ModelArtifacts\"\n",
      "              },\n",
      "              \"LocalPath\": \"/opt/ml/processing/model\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"InputName\": \"input-2\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/electronics/data/split_cicd\",\n",
      "              \"LocalPath\": \"/opt/ml/processing/input/code/datasets\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"InputName\": \"code\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/049bf5e9d232ff7d9a27fce88e210f56/sourcedir.tar.gz\",\n",
      "              \"LocalPath\": \"/opt/ml/processing/input/code/\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"InputName\": \"entrypoint\",\n",
      "            \"AppManaged\": false,\n",
      "            \"S3Input\": {\n",
      "              \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\",\n",
      "              \"LocalPath\": \"/opt/ml/processing/input/entrypoint\",\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3InputMode\": \"File\",\n",
      "              \"S3DataDistributionType\": \"FullyReplicated\",\n",
      "              \"S3CompressionType\": \"None\"\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"ProcessingOutputConfig\": {\n",
      "          \"Outputs\": [\n",
      "            {\n",
      "              \"OutputName\": \"evaluation\",\n",
      "              \"AppManaged\": false,\n",
      "              \"S3Output\": {\n",
      "                \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/electronics-eval-2024-07-01-15-09-11-221/output/evaluation\",\n",
      "                \"LocalPath\": \"/opt/ml/processing/evaluation\",\n",
      "                \"S3UploadMode\": \"EndOfJob\"\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      \"PropertyFiles\": [\n",
      "        {\n",
      "          \"PropertyFileName\": \"EvaluationReport\",\n",
      "          \"OutputName\": \"evaluation\",\n",
      "          \"FilePath\": \"evaluation.json\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"ElectronicsMAPCond\",\n",
      "      \"Type\": \"Condition\",\n",
      "      \"Arguments\": {\n",
      "        \"Conditions\": [\n",
      "          {\n",
      "            \"Type\": \"GreaterThanOrEqualTo\",\n",
      "            \"LeftValue\": {\n",
      "              \"Std:JsonGet\": {\n",
      "                \"PropertyFile\": {\n",
      "                  \"Get\": \"Steps.ElectronicsEval.PropertyFiles.EvaluationReport\"\n",
      "                },\n",
      "                \"Path\": \"detection_metrics.mAP50.value\"\n",
      "              }\n",
      "            },\n",
      "            \"RightValue\": {\n",
      "              \"Get\": \"Parameters.mAPThreshold\"\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"IfSteps\": [\n",
      "          {\n",
      "            \"Name\": \"ElectronicsRegisterModel-RepackModel-0\",\n",
      "            \"Type\": \"Training\",\n",
      "            \"Arguments\": {\n",
      "              \"AlgorithmSpecification\": {\n",
      "                \"TrainingInputMode\": \"File\",\n",
      "                \"TrainingImage\": \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\"\n",
      "              },\n",
      "              \"OutputDataConfig\": {\n",
      "                \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/pytorch-inference-2024-07-01-15-09-11-483\"\n",
      "              },\n",
      "              \"StoppingCondition\": {\n",
      "                \"MaxRuntimeInSeconds\": 86400\n",
      "              },\n",
      "              \"ResourceConfig\": {\n",
      "                \"VolumeSizeInGB\": 30,\n",
      "                \"InstanceCount\": 1,\n",
      "                \"InstanceType\": \"ml.m5.large\"\n",
      "              },\n",
      "              \"RoleArn\": \"arn:aws:iam::676076160400:role/LabRole\",\n",
      "              \"InputDataConfig\": [\n",
      "                {\n",
      "                  \"DataSource\": {\n",
      "                    \"S3DataSource\": {\n",
      "                      \"S3DataType\": \"S3Prefix\",\n",
      "                      \"S3Uri\": {\n",
      "                        \"Get\": \"Steps.ElectronicsTrain.ModelArtifacts.S3ModelArtifacts\"\n",
      "                      },\n",
      "                      \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                    }\n",
      "                  },\n",
      "                  \"ChannelName\": \"training\"\n",
      "                }\n",
      "              ],\n",
      "              \"HyperParameters\": {\n",
      "                \"inference_script\": \"\\\"inference.py\\\"\",\n",
      "                \"model_archive\": {\n",
      "                  \"Std:Join\": {\n",
      "                    \"On\": \"\",\n",
      "                    \"Values\": [\n",
      "                      {\n",
      "                        \"Get\": \"Steps.ElectronicsTrain.ModelArtifacts.S3ModelArtifacts\"\n",
      "                      }\n",
      "                    ]\n",
      "                  }\n",
      "                },\n",
      "                \"dependencies\": \"null\",\n",
      "                \"source_dir\": \"\\\"code\\\"\",\n",
      "                \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-676076160400/ElectronicsRegisterModel-RepackModel-0-6a7387937606cba3f7c37364a98bbe32/source/sourcedir.tar.gz\\\"\",\n",
      "                \"sagemaker_program\": \"\\\"_repack_script_launcher.sh\\\"\",\n",
      "                \"sagemaker_container_log_level\": \"20\",\n",
      "                \"sagemaker_region\": \"\\\"us-east-1\\\"\"\n",
      "              },\n",
      "              \"DebugHookConfig\": {\n",
      "                \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/pytorch-inference-2024-07-01-15-09-11-483\",\n",
      "                \"CollectionConfigurations\": []\n",
      "              },\n",
      "              \"ProfilerConfig\": {\n",
      "                \"DisableProfiler\": true\n",
      "              }\n",
      "            },\n",
      "            \"Description\": \"Used to repack a model with customer scripts for a register/create model step\"\n",
      "          },\n",
      "          {\n",
      "            \"Name\": \"ElectronicsRegisterModel-RegisterModel\",\n",
      "            \"Type\": \"RegisterModel\",\n",
      "            \"Arguments\": {\n",
      "              \"ModelPackageGroupName\": \"ElectronicsModelPackageGroupName\",\n",
      "              \"ModelMetrics\": {\n",
      "                \"ModelQuality\": {\n",
      "                  \"Statistics\": {\n",
      "                    \"ContentType\": \"application/json\",\n",
      "                    \"S3Uri\": \"s3://sagemaker-us-east-1-676076160400/electronics-eval-2024-07-01-15-09-11-221/output/evaluation/evaluation.json\"\n",
      "                  }\n",
      "                },\n",
      "                \"Bias\": {},\n",
      "                \"Explainability\": {}\n",
      "              },\n",
      "              \"Domain\": \"COMPUTER_VISION\",\n",
      "              \"InferenceSpecification\": {\n",
      "                \"Containers\": [\n",
      "                  {\n",
      "                    \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.1.0-cpu-py310\",\n",
      "                    \"Environment\": {\n",
      "                      \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "                      \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code\",\n",
      "                      \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
      "                      \"SAGEMAKER_REGION\": \"us-east-1\"\n",
      "                    },\n",
      "                    \"ModelDataUrl\": {\n",
      "                      \"Get\": \"Steps.ElectronicsRegisterModel-RepackModel-0.ModelArtifacts.S3ModelArtifacts\"\n",
      "                    },\n",
      "                    \"Framework\": \"PYTORCH\",\n",
      "                    \"FrameworkVersion\": \"1.3\"\n",
      "                  }\n",
      "                ],\n",
      "                \"SupportedContentTypes\": [\n",
      "                  \"image/jpeg\"\n",
      "                ],\n",
      "                \"SupportedResponseMIMETypes\": [\n",
      "                  \"application/json\"\n",
      "                ],\n",
      "                \"SupportedRealtimeInferenceInstanceTypes\": [\n",
      "                  \"ml.t2.medium\",\n",
      "                  \"ml.m5.xlarge\"\n",
      "                ],\n",
      "                \"SupportedTransformInstanceTypes\": [\n",
      "                  \"ml.m5.xlarge\"\n",
      "                ]\n",
      "              },\n",
      "              \"ModelApprovalStatus\": {\n",
      "                \"Get\": \"Parameters.ModelApprovalStatus\"\n",
      "              },\n",
      "              \"SkipModelValidation\": \"None\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"Name\": \"ElectronicsCreateModel-RepackModel-0\",\n",
      "            \"Type\": \"Training\",\n",
      "            \"Arguments\": {\n",
      "              \"AlgorithmSpecification\": {\n",
      "                \"TrainingInputMode\": \"File\",\n",
      "                \"TrainingImage\": \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\"\n",
      "              },\n",
      "              \"OutputDataConfig\": {\n",
      "                \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/pytorch-inference-2024-07-01-15-08-29-706\"\n",
      "              },\n",
      "              \"StoppingCondition\": {\n",
      "                \"MaxRuntimeInSeconds\": 86400\n",
      "              },\n",
      "              \"ResourceConfig\": {\n",
      "                \"VolumeSizeInGB\": 30,\n",
      "                \"InstanceCount\": 1,\n",
      "                \"InstanceType\": \"ml.m5.large\"\n",
      "              },\n",
      "              \"RoleArn\": \"arn:aws:iam::676076160400:role/LabRole\",\n",
      "              \"InputDataConfig\": [\n",
      "                {\n",
      "                  \"DataSource\": {\n",
      "                    \"S3DataSource\": {\n",
      "                      \"S3DataType\": \"S3Prefix\",\n",
      "                      \"S3Uri\": {\n",
      "                        \"Get\": \"Steps.ElectronicsTrain.ModelArtifacts.S3ModelArtifacts\"\n",
      "                      },\n",
      "                      \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                    }\n",
      "                  },\n",
      "                  \"ChannelName\": \"training\"\n",
      "                }\n",
      "              ],\n",
      "              \"HyperParameters\": {\n",
      "                \"inference_script\": \"\\\"inference.py\\\"\",\n",
      "                \"model_archive\": {\n",
      "                  \"Std:Join\": {\n",
      "                    \"On\": \"\",\n",
      "                    \"Values\": [\n",
      "                      {\n",
      "                        \"Get\": \"Steps.ElectronicsTrain.ModelArtifacts.S3ModelArtifacts\"\n",
      "                      }\n",
      "                    ]\n",
      "                  }\n",
      "                },\n",
      "                \"dependencies\": \"null\",\n",
      "                \"source_dir\": \"\\\"code\\\"\",\n",
      "                \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-us-east-1-676076160400/ElectronicsCreateModel-RepackModel-0-6a7387937606cba3f7c37364a98bbe32/source/sourcedir.tar.gz\\\"\",\n",
      "                \"sagemaker_program\": \"\\\"_repack_script_launcher.sh\\\"\",\n",
      "                \"sagemaker_container_log_level\": \"20\",\n",
      "                \"sagemaker_region\": \"\\\"us-east-1\\\"\"\n",
      "              },\n",
      "              \"DebugHookConfig\": {\n",
      "                \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/pytorch-inference-2024-07-01-15-08-29-706\",\n",
      "                \"CollectionConfigurations\": []\n",
      "              },\n",
      "              \"ProfilerConfig\": {\n",
      "                \"DisableProfiler\": true\n",
      "              }\n",
      "            },\n",
      "            \"Description\": \"Used to repack a model with customer scripts for a register/create model step\"\n",
      "          },\n",
      "          {\n",
      "            \"Name\": \"ElectronicsCreateModel-CreateModel\",\n",
      "            \"Type\": \"Model\",\n",
      "            \"Arguments\": {\n",
      "              \"ExecutionRoleArn\": \"arn:aws:iam::676076160400:role/LabRole\",\n",
      "              \"PrimaryContainer\": {\n",
      "                \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.1.0-cpu-py310\",\n",
      "                \"Environment\": {\n",
      "                  \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "                  \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code\",\n",
      "                  \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
      "                  \"SAGEMAKER_REGION\": \"us-east-1\"\n",
      "                },\n",
      "                \"ModelDataUrl\": {\n",
      "                  \"Get\": \"Steps.ElectronicsCreateModel-RepackModel-0.ModelArtifacts.S3ModelArtifacts\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"Name\": \"ElectronicsTransform\",\n",
      "            \"Type\": \"Transform\",\n",
      "            \"Arguments\": {\n",
      "              \"ModelName\": {\n",
      "                \"Get\": \"Steps.ElectronicsCreateModel-CreateModel.ModelName\"\n",
      "              },\n",
      "              \"TransformInput\": {\n",
      "                \"DataSource\": {\n",
      "                  \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": {\n",
      "                      \"Get\": \"Parameters.BatchData\"\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              },\n",
      "              \"TransformOutput\": {\n",
      "                \"S3OutputPath\": \"s3://sagemaker-us-east-1-676076160400/ElectronicsTransform\",\n",
      "                \"Accept\": \"application/json\"\n",
      "              },\n",
      "              \"TransformResources\": {\n",
      "                \"InstanceCount\": 1,\n",
      "                \"InstanceType\": \"ml.m5.xlarge\"\n",
      "              },\n",
      "              \"MaxPayloadInMB\": 10\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"ElseSteps\": [\n",
      "          {\n",
      "            \"Name\": \"ElectronicsMAPFail\",\n",
      "            \"Type\": \"Fail\",\n",
      "            \"Arguments\": {\n",
      "              \"ErrorMessage\": {\n",
      "                \"Std:Join\": {\n",
      "                  \"On\": \" \",\n",
      "                  \"Values\": [\n",
      "                    \"Execution failed due to mAP <\",\n",
      "                    {\n",
      "                      \"Get\": \"Parameters.mAPThreshold\"\n",
      "                    }\n",
      "                  ]\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.get_training_image_uri) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.processing:Uploaded code to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/049bf5e9d232ff7d9a27fce88e210f56/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-676076160400/ElectronicsPipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreatePipeline operation: Unknown property reference [Parameters.InputData].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(definition, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Submit the pipeline to SageMaker, start execution\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole_arn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m execution \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Wait for the pipeline execution to complete\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline.py:297\u001b[0m, in \u001b[0;36mPipeline.upsert\u001b[0;34m(self, role_arn, description, tags, parallelism_config)\u001b[0m\n\u001b[1;32m    295\u001b[0m error_message \u001b[38;5;241m=\u001b[39m ce\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidationException\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready exists\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message):\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ce\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# already exists\u001b[39;00m\n\u001b[1;32m    299\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(role_arn, description, parallelism_config\u001b[38;5;241m=\u001b[39mparallelism_config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline.py:292\u001b[0m, in \u001b[0;36mPipeline.upsert\u001b[0;34m(self, role_arn, description, tags, parallelism_config)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn AWS IAM role is required to create or update a Pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole_arn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallelism_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m ce:\n\u001b[1;32m    294\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m ce\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline.py:169\u001b[0m, in \u001b[0;36mPipeline.create\u001b[0;34m(self, role_arn, description, tags, parallelism_config)\u001b[0m\n\u001b[1;32m    164\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_args(role_arn, description, parallelism_config)\n\u001b[1;32m    165\u001b[0m update_args(\n\u001b[1;32m    166\u001b[0m     kwargs,\n\u001b[1;32m    167\u001b[0m     Tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    168\u001b[0m )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreatePipeline operation: Unknown property reference [Parameters.InputData]."
     ]
    }
   ],
   "source": [
    "pipeline_name = \"ElectronicsPipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        map_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")\n",
    "\n",
    "# Examine the pipeline definition\n",
    "import json\n",
    "definition = json.loads(pipeline.definition())\n",
    "print(json.dumps(definition, indent=2))\n",
    "\n",
    "# Submit the pipeline to SageMaker, start execution\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "\n",
    "# Wait for the pipeline execution to complete\n",
    "execution.wait()\n",
    "\n",
    "# List steps in the pipeline execution\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "611f2803-fc84-4318-b7c2-91f7a3b8f332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Name: ElectronicsTrain\n",
      "Status: Failed\n",
      "------\n",
      "Step Name: ElectronicsProcess\n",
      "Status: Failed\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "execution_steps = execution.list_steps()\n",
    "for step in execution_steps:\n",
    "    print(f\"Step Name: {step['StepName']}\")\n",
    "    print(f\"Status: {step['StepStatus']}\")\n",
    "    if 'FailureReason' in step['Metadata']:\n",
    "        print(f\"Failure Reason: {step['Metadata']['FailureReason']}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40f2aa14-1436-4a9d-a91f-1cb0ab91cf85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving logs for job: sagemaker-xgboost-2024-06--ProfilerReport-1719730592-f9d4972a\n",
      "Fetching logs for log stream: sagemaker-xgboost-2024-06--ProfilerReport-1719730592-f9d4972a/algo-1-1719730632\n",
      "[2024-06-30 06:57:40.695 ip-10-0-152-66.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1719730592\n",
      "[2024-06-30 06:57:41.325 ip-10-0-152-66.ec2.internal:7 INFO profiler_trial.py:67] Waiting for profiler data.\n",
      "[2024-06-30 06:57:51.335 ip-10-0-152-66.ec2.internal:7 INFO profiler_trial.py:67] Waiting for profiler data.\n",
      "[2024-06-30 06:58:01.343 ip-10-0-152-66.ec2.internal:7 INFO profiler_trial.py:67] Waiting for profiler data.\n",
      "[2024-06-30 06:58:51.386 ip-10-0-152-66.ec2.internal:7 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\n",
      "No environment variable found with name \"base_trial\". Will use default param value if present\n",
      "No environment variable found with name \"scan_interval_us\". Will use default param value if present\n",
      "No environment variable found with name \"nb_path\". Will use default param value if present\n",
      "No environment variable found with name \"opt_out_telemetry\". Will use default param value if present\n",
      "No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule ProfilerReport.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule BatchSize.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule CPUBottleneck.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule Dataloader.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule GPUMemoryIncrease.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule IOBottleneck.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule LoadBalancing.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule LowGPUUtilization.\n",
      "[2024-06-30 06:58:51.387 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule MaxInitializationTime.\n",
      "[2024-06-30 06:58:51.388 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule OverallSystemUsage.\n",
      "[2024-06-30 06:58:51.388 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule OverallFrameworkMetrics.\n",
      "[2024-06-30 06:58:51.388 ip-10-0-152-66.ec2.internal:7 INFO action.py:20] No action specified for rule StepOutlier.\n",
      "[2024-06-30 06:58:51.388 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:104] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\n",
      "job_config:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:676076160400:processing-job/sagemaker-xgboost-2024-06--ProfilerReport-1719730592-f9d4972a', 'ProcessingJobName': 'sagemaker-xgboost-2024-06--ProfilerReport-1719730592-f9d4972a', 'Environment': {'SAGEMAKER_ENV_RULE_STOP_SIGNAL_FILE': '/opt/ml/processing/input/profiler/signals/ProfilerReport-1719730592', 'SAGEMAKER_START_TIMESTAMP_OF_RULE': '1719730594', 'TRAINING_IMAGE': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.2-1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:676076160400:training-job/sagemaker-xgboost-2024-06-30-06-56-32-730', 'rule_to_invoke': 'ProfilerReport'}, 'AppSpecification': {'ImageUri': '503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'Profiler', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/profiler', 'S3Uri': 's3://sagemaker-us-east-1-676076160400/sagemaker-xgboost-2024-06-30-06-56-32-730/profiler-output', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'Continuous'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'RuleOutput', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output/rule', 'S3Uri': 's3://sagemaker-us-east-1-676076160400/sagemaker-xgboost-2024-06-30-06-56-32-730/rule-output/ProfilerReport-1719730592', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.2xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::676076160400:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\n",
      "[2024-06-30 06:58:51.388 ip-10-0-152-66.ec2.internal:7 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\n",
      "[2024-06-30 06:58:51.389 ip-10-0-152-66.ec2.internal:7 INFO metrics_reader_base.py:134] Getting 2 event files\n",
      "[2024-06-30 06:58:51.409 ip-10-0-152-66.ec2.internal:7 INFO metrics_reader_base.py:134] Getting 0 event files\n",
      "[2024-06-30 06:58:51.410 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:BatchSize for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.411 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:CPUBottleneck for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.413 ip-10-0-152-66.ec2.internal:7 INFO cpu_bottleneck.py:171] Found 0 CPU bottlenecks\n",
      "[2024-06-30 06:58:51.413 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:Dataloader for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.413 ip-10-0-152-66.ec2.internal:7 INFO dataloader.py:192] No dataloading metrics found.\n",
      "[2024-06-30 06:58:51.413 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:GPUMemoryIncrease for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.414 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:IOBottleneck for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.415 ip-10-0-152-66.ec2.internal:7 INFO io_bottleneck.py:170] Found 0 IO bottlenecks\n",
      "[2024-06-30 06:58:51.416 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:LoadBalancing for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.416 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:LowGPUUtilization for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.416 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:MaxInitializationTime for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.418 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:OverallSystemUsage for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.421 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:OverallFrameworkMetrics for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "[2024-06-30 06:58:51.422 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:200] Invoking rule:StepOutlier for timestamp_start:1719730620000000 to timestamp_end:1719730680000000\n",
      "#015Executing:   0%|          | 0/33 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\n",
      "/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\n",
      "[2024-06-30 06:58:55.838 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:135] No exceptions raised while executing notebook\n",
      "[2024-06-30 06:58:55.838 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:138] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\n",
      "[2024-06-30 06:58:55.838 ip-10-0-152-66.ec2.internal:7 INFO profiler_report.py:143] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\n",
      "[2024-06-30 06:58:56.322 ip-10-0-152-66.ec2.internal:7 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1719730740000000\n",
      "[2024-06-30 06:58:56.322 ip-10-0-152-66.ec2.internal:7 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\n",
      "#015Executing:   3%|▎         | 1/33 [00:00<00:29,  1.09cell/s]#015Executing:   6%|▌         | 2/33 [00:01<00:28,  1.10cell/s]#015Executing:   9%|▉         | 3/33 [00:02<00:18,  1.66cell/s]#015Executing:  24%|██▍       | 8/33 [00:02<00:03,  6.29cell/s]#015Executing:  36%|███▋      | 12/33 [00:02<00:02, 10.15cell/s]#015Executing:  48%|████▊     | 16/33 [00:02<00:01, 14.05cell/s]#015Executing:  58%|█████▊    | 19/33 [00:02<00:00, 16.65cell/s]#015Executing:  70%|██████▉   | 23/33 [00:02<00:00, 20.75cell/s]#015Executing:  82%|████████▏ | 27/33 [00:02<00:00, 22.04cell/s]#015Executing:  91%|█████████ | 30/33 [00:02<00:00, 21.62cell/s]#015Executing: 100%|██████████| 33/33 [00:03<00:00, 18.89cell/s]#015Executing: 100%|██████████| 33/33 [00:04<00:00,  7.72cell/s]\n",
      "Rule evaluation complete.\n",
      "Retrieving logs for job: xgboost-2024-06-30-22-29-23-246\n",
      "Fetching logs for log stream: xgboost-2024-06-30-22-29-23-246/algo-1-1719786605\n",
      "Arguments: train\n",
      "[2024-06-30:22:31:19:INFO] Running standalone xgboost training.\n",
      "[2024-06-30:22:31:19:INFO] File size need to be processed in the node: 0.16mb. Available memory size in the node: 8137.62mb\n",
      "[22:31:19] S3DistributionType set as FullyReplicated\n",
      "[22:31:19] 2850x5 matrix with 11400 entries loaded from /opt/ml/input/data/train\n",
      "[22:31:19] S3DistributionType set as FullyReplicated\n",
      "[22:31:19] 2138x5 matrix with 8552 entries loaded from /opt/ml/input/data/validation\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[0]#011train-error:0.126667#011validation-error:0.148269\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 68 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]#011train-error:0.123158#011validation-error:0.14406\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]#011train-error:0.124211#011validation-error:0.141721\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[3]#011train-error:0.122807#011validation-error:0.141254\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]#011train-error:0.121053#011validation-error:0.140318\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 80 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]#011train-error:0.123158#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]#011train-error:0.120702#011validation-error:0.138915\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]#011train-error:0.119298#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]#011train-error:0.120351#011validation-error:0.13985\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]#011train-error:0.120351#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[10]#011train-error:0.120351#011validation-error:0.13985\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11]#011train-error:0.119298#011validation-error:0.13985\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[12]#011train-error:0.118596#011validation-error:0.13985\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[13]#011train-error:0.118246#011validation-error:0.13985\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[14]#011train-error:0.116842#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[15]#011train-error:0.117193#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[16]#011train-error:0.115439#011validation-error:0.138447\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[17]#011train-error:0.115439#011validation-error:0.137979\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[18]#011train-error:0.115439#011validation-error:0.138447\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[19]#011train-error:0.115789#011validation-error:0.138447\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[20]#011train-error:0.114386#011validation-error:0.138447\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[21]#011train-error:0.114737#011validation-error:0.138447\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[22]#011train-error:0.115088#011validation-error:0.140318\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[23]#011train-error:0.114035#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[24]#011train-error:0.112632#011validation-error:0.137979\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[25]#011train-error:0.112632#011validation-error:0.137979\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[26]#011train-error:0.110877#011validation-error:0.138447\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[27]#011train-error:0.110526#011validation-error:0.138915\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[28]#011train-error:0.109474#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[29]#011train-error:0.109474#011validation-error:0.139383\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[30]#011train-error:0.109123#011validation-error:0.13985\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[31]#011train-error:0.107368#011validation-error:0.141254\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[32]#011train-error:0.107018#011validation-error:0.140786\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[33]#011train-error:0.105614#011validation-error:0.141254\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[34]#011train-error:0.105614#011validation-error:0.141254\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[35]#011train-error:0.104561#011validation-error:0.142189\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[36]#011train-error:0.102105#011validation-error:0.143124\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[37]#011train-error:0.102105#011validation-error:0.143592\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[38]#011train-error:0.101754#011validation-error:0.143592\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[39]#011train-error:0.101404#011validation-error:0.14406\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[40]#011train-error:0.100702#011validation-error:0.14406\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[41]#011train-error:0.098947#011validation-error:0.144528\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[42]#011train-error:0.098947#011validation-error:0.144528\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[43]#011train-error:0.098596#011validation-error:0.144995\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[44]#011train-error:0.097544#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[45]#011train-error:0.097193#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[46]#011train-error:0.096491#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[47]#011train-error:0.09614#011validation-error:0.147802\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[48]#011train-error:0.09614#011validation-error:0.147802\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[49]#011train-error:0.095789#011validation-error:0.147802\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[50]#011train-error:0.095789#011validation-error:0.147802\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[51]#011train-error:0.095439#011validation-error:0.147802\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[52]#011train-error:0.095439#011validation-error:0.147802\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[53]#011train-error:0.095088#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[54]#011train-error:0.095439#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[55]#011train-error:0.095088#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[56]#011train-error:0.094737#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[57]#011train-error:0.094386#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[58]#011train-error:0.094737#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[59]#011train-error:0.095088#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[60]#011train-error:0.093684#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[61]#011train-error:0.093684#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[62]#011train-error:0.092982#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[63]#011train-error:0.09193#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[64]#011train-error:0.090526#011validation-error:0.145931\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[65]#011train-error:0.089123#011validation-error:0.145463\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[66]#011train-error:0.088421#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[67]#011train-error:0.088421#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[68]#011train-error:0.088421#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[69]#011train-error:0.087719#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[70]#011train-error:0.08807#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[71]#011train-error:0.087368#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[72]#011train-error:0.086316#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[73]#011train-error:0.086316#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[74]#011train-error:0.086667#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[75]#011train-error:0.086667#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[76]#011train-error:0.086316#011validation-error:0.145931\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[77]#011train-error:0.085614#011validation-error:0.145931\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[78]#011train-error:0.085263#011validation-error:0.145931\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[79]#011train-error:0.085965#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[80]#011train-error:0.085614#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[81]#011train-error:0.08386#011validation-error:0.146398\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[82]#011train-error:0.083158#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[83]#011train-error:0.082807#011validation-error:0.146866\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[84]#011train-error:0.082807#011validation-error:0.147334\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[85]#011train-error:0.082807#011validation-error:0.148269\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[86]#011train-error:0.082105#011validation-error:0.148269\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[87]#011train-error:0.081754#011validation-error:0.148737\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[88]#011train-error:0.080351#011validation-error:0.148269\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[89]#011train-error:0.081053#011validation-error:0.148737\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[90]#011train-error:0.081053#011validation-error:0.149205\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[91]#011train-error:0.080351#011validation-error:0.148737\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[92]#011train-error:0.079649#011validation-error:0.149673\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[93]#011train-error:0.08#011validation-error:0.15014\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[94]#011train-error:0.078246#011validation-error:0.149673\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[95]#011train-error:0.077895#011validation-error:0.151076\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[96]#011train-error:0.077895#011validation-error:0.150608\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[97]#011train-error:0.078246#011validation-error:0.15014\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[98]#011train-error:0.078246#011validation-error:0.15014\n",
      "[22:31:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[99]#011train-error:0.078596#011validation-error:0.150608\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "log_client = boto3.client('logs')\n",
    "\n",
    "def list_log_streams(log_group_name, job_name_prefix):\n",
    "    response = log_client.describe_log_streams(\n",
    "        logGroupName=log_group_name,\n",
    "        logStreamNamePrefix=job_name_prefix\n",
    "    )\n",
    "    log_streams = response.get('logStreams', [])\n",
    "    return log_streams\n",
    "\n",
    "def get_logs(log_group_name, log_stream_name):\n",
    "    response = log_client.get_log_events(\n",
    "        logGroupName=log_group_name,\n",
    "        logStreamName=log_stream_name,\n",
    "        startFromHead=True\n",
    "    )\n",
    "    for event in response['events']:\n",
    "        print(event['message'])\n",
    "\n",
    "def retrieve_logs_for_job(log_group_name, job_name):\n",
    "    print(f\"Retrieving logs for job: {job_name}\")\n",
    "    log_streams = list_log_streams(log_group_name, job_name)\n",
    "    if not log_streams:\n",
    "        print(f\"No log streams found for job: {job_name}\")\n",
    "    for log_stream in log_streams:\n",
    "        print(f\"Fetching logs for log stream: {log_stream['logStreamName']}\")\n",
    "        get_logs(log_group_name, log_stream['logStreamName'])\n",
    "\n",
    "# Define the log group names for processing and training jobs\n",
    "processing_log_group = '/aws/sagemaker/ProcessingJobs'\n",
    "training_log_group = '/aws/sagemaker/TrainingJobs'\n",
    "\n",
    "# Define your job names\n",
    "processing_job_name = 'sagemaker-xgboost-2024-06--ProfilerReport-1719730592-f9d4972a'\n",
    "training_job_name = 'xgboost-2024-06-30-22-29-23-246'\n",
    "\n",
    "# Retrieve logs for the processing job\n",
    "retrieve_logs_for_job(processing_log_group, processing_job_name)\n",
    "\n",
    "# Retrieve logs for the training job\n",
    "retrieve_logs_for_job(training_log_group, training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346dce6-9542-4ad7-b484-6bc767d58dfc",
   "metadata": {},
   "source": [
    "### Create and Schedule Data Quality Monitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2917fc85-3b13-4ec0-b580-7d9e944b0abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electronics-dataset\n",
      "sagemaker-studio-676076160400-3rt69i74mgv\n",
      "sagemaker-us-east-1-676076160400\n",
      "No objects found in electronics-dataset/your-prefix-path/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Print out bucket names\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])\n",
    "\n",
    "bucket_name = 'electronics-dataset'\n",
    "prefix = 'your-prefix-path/'\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(f\"No objects found in {bucket_name}/{prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57e2292b-8ef4-49d2-98f2-c2ea1a7bc9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::676076160400:role/LabRole\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bbdbbb6-f65e-401f-ad5e-b8ae025cd236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders created successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'electronics-dataset'\n",
    "\n",
    "# Create folders\n",
    "folders = ['sagemaker/benchmark-model/batch-data/', 'sagemaker/benchmark-model/reports/']\n",
    "\n",
    "for folder in folders:\n",
    "    s3.put_object(Bucket=bucket_name, Key=(folder + '/'))\n",
    "\n",
    "print(\"Folders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdb28ebe-b358-4eb2-9863-a9f7a561fc38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2024-07-01-15-39-42-219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............\u001b[34m2024-07-01 15:41:39.954762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:39.954793: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:41.591716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:41.591746: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:41.591764: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-105-216.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:41.592036: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,194 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:676076160400:processing-job/baseline-suggestion-job-2024-07-01-15-39-42-219', 'ProcessingJobName': 'baseline-suggestion-job-2024-07-01-15-39-42-219', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://electronics-dataset/DatafinitiElectronicsProductData.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://electronics-dataset/sagemaker/benchmark-model/output//baseline_results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::676076160400:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,194 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,194 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,194 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,194 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,194 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,242 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,242 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,243 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,250 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,251 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,251 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,714 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.105.216\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-\u001b[0m\n",
      "\u001b[34mnodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,721 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:43,725 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-148f8726-4560-48b3-a208-dacda1c7f28c\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,299 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,314 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,315 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,318 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,324 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,324 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,324 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,324 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,368 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,382 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,382 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,388 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,391 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Jul 01 15:41:44\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,393 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,393 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,395 INFO util.GSet: 2.0% max memory 3.1 GB = 62.7 MB\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,395 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,432 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,435 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,436 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,461 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,461 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,461 INFO util.GSet: 1.0% max memory 3.1 GB = 31.3 MB\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,461 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,463 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,463 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,463 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,464 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,468 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,471 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,471 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,471 INFO util.GSet: 0.25% max memory 3.1 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,471 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,508 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,508 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,508 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,511 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,512 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,514 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,514 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,514 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 962.9 KB\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,514 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,533 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1040720000-10.2.105.216-1719848504527\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,545 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,553 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,635 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,647 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,651 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.105.216\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:44,661 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:46,721 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:46,722 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:48,795 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:48,795 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:50,912 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:50,912 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:53,022 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:53,023 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:55,222 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:41:55,223 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:05,230 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:06,949 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,369 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,415 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,426 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,960 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,987 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,987 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,988 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:07,988 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,011 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,025 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,027 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,079 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,079 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,079 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,080 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,080 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,422 INFO util.Utils: Successfully started service 'sparkDriver' on port 40229.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,453 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,492 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,515 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,515 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,549 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,575 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-2ed16f87-bdfd-4d47-97c0-ee97fc9bc4be\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,598 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,640 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:08,675 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.105.216:40229/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1719848527956\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,202 INFO client.RMProxy: Connecting to ResourceManager at /10.2.105.216:8032\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,887 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,887 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,894 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,894 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,894 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,895 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,900 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:09,982 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:11,582 INFO yarn.Client: Uploading resource file:/tmp/spark-02e930eb-c281-4ba6-8c1e-bcd655264d50/__spark_libs__6124116765450356618.zip -> hdfs://10.2.105.216/user/root/.sparkStaging/application_1719848510594_0001/__spark_libs__6124116765450356618.zip\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,723 INFO yarn.Client: Uploading resource file:/tmp/spark-02e930eb-c281-4ba6-8c1e-bcd655264d50/__spark_conf__5969664138424828990.zip -> hdfs://10.2.105.216/user/root/.sparkStaging/application_1719848510594_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,777 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,778 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,778 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,778 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,778 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:12,807 INFO yarn.Client: Submitting application application_1719848510594_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:13,003 INFO impl.YarnClientImpl: Submitted application application_1719848510594_0001\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:14,008 INFO yarn.Client: Application report for application_1719848510594_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:14,012 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Jul 01 15:42:13 +0000 2024] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1719848532907\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1719848510594_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:15,016 INFO yarn.Client: Application report for application_1719848510594_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:16,019 INFO yarn.Client: Application report for application_1719848510594_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:17,022 INFO yarn.Client: Application report for application_1719848510594_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:18,026 INFO yarn.Client: Application report for application_1719848510594_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:18,501 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1719848510594_0001), /proxy/application_1719848510594_0001\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,030 INFO yarn.Client: Application report for application_1719848510594_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,032 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.105.216\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1719848532907\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1719848510594_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,037 INFO cluster.YarnClientSchedulerBackend: Application application_1719848510594_0001 has started running.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,067 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33659.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,067 INFO netty.NettyBlockTransferService: Server created on 10.2.105.216:33659\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,070 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,081 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.105.216, 33659, None)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,086 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.105.216:33659 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.105.216, 33659, None)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,089 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.105.216, 33659, None)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,091 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.105.216, 33659, None)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:19,283 INFO util.log: Logging initialized @13815ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:20,029 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:24,524 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.105.216:34162) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:24,711 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:42537 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 42537, None)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:39,154 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:39,372 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:39,429 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:39,434 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:40,547 INFO datasources.InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:40,701 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 417.0 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,044 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,047 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.105.216:33659 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,052 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,436 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,439 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,444 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,496 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,512 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,512 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,512 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,514 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,519 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,546 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,550 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,551 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.105.216:33659 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,551 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,570 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,572 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,621 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4645 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:41,883 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:42537 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:42,748 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:42537 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,118 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1514 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,120 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,126 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.586 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,129 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,130 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,131 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.635322 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,306 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.105.216:33659 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:43,308 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:42537 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,448 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,449 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,452 INFO datasources.FileSourceStrategy: Output Data Schema: struct<id: string, asins: string, brand: string, categories: string, colors: string ... 25 more fields>\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,491 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,664 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,679 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,680 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.105.216:33659 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,681 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,695 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 22064752 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,740 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,742 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,742 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,742 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,745 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,751 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,802 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,804 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,804 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.105.216:33659 (size: 9.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,805 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,806 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,807 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,811 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4973 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:45,859 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:42537 (size: 9.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:46,701 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:42537 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,779 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:42537 (size: 4.2 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,908 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,908 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,909 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.155 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,909 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,910 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:47,910 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.169773 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,216 INFO codegen.CodeGenerator: Code generated in 216.151785 ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,880 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,885 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,885 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,886 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,888 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,890 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,910 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 117.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,913 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,913 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.105.216:33659 (size: 35.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,914 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,916 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,916 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,924 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:48,950 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:42537 (size: 35.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,361 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1439 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,364 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,367 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.474 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,368 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,369 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,369 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,370 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,458 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,460 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,460 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,460 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,460 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,461 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,474 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,476 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,476 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.105.216:33659 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,477 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,477 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,477 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,480 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,496 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:42537 (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,538 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,886 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 407 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,887 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,888 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.419 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,888 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,888 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,888 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.430560 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:50,933 INFO codegen.CodeGenerator: Code generated in 35.164744 ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,078 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.105.216:33659 in memory (size: 9.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,085 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:42537 in memory (size: 9.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,122 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.105.216:33659 in memory (size: 46.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,123 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:42537 in memory (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,129 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,132 INFO scheduler.DAGScheduler: Registering RDD 27 (countByKey at ColumnProfiler.scala:592) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,133 INFO scheduler.DAGScheduler: Got job 4 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,133 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,133 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,133 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,138 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[27] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,155 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:42537 in memory (size: 35.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,162 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.105.216:33659 in memory (size: 35.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,185 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 33.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,187 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,188 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.105.216:33659 (size: 15.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,189 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,189 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[27] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,189 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,191 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:51,210 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:42537 (size: 15.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,763 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1572 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,763 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,765 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (countByKey at ColumnProfiler.scala:592) finished in 1.624 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,765 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,765 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,765 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,765 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,765 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (ShuffledRDD[28] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,769 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,771 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,774 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.105.216:33659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,775 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,775 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRDD[28] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,778 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,781 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,801 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:42537 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,816 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,904 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 124 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,904 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,906 INFO scheduler.DAGScheduler: ResultStage 6 (countByKey at ColumnProfiler.scala:592) finished in 0.137 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,906 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,906 INFO cluster.YarnScheduler: Killing all running tasks in stage 6: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:52,907 INFO scheduler.DAGScheduler: Job 4 finished: countByKey at ColumnProfiler.scala:592, took 1.778301 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,189 INFO scheduler.DAGScheduler: Registering RDD 33 (collect at AnalysisRunner.scala:326) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,189 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,190 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 7 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,190 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,191 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,191 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,199 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 86.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,201 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,202 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.105.216:33659 (size: 28.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,202 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,203 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,205 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,207 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,219 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:42537 (size: 28.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,505 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 298 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,505 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,507 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (collect at AnalysisRunner.scala:326) finished in 0.312 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,507 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,507 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,507 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,507 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,563 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,565 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,565 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,565 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,565 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,566 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[36] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,575 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 171.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,577 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 47.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,578 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.105.216:33659 (size: 47.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,578 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,579 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[36] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,579 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,580 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,594 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:42537 (size: 47.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,604 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,703 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 123 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,704 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,705 INFO scheduler.DAGScheduler: ResultStage 9 (collect at AnalysisRunner.scala:326) finished in 0.138 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,705 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,705 INFO cluster.YarnScheduler: Killing all running tasks in stage 9: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,705 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.142086 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,857 INFO codegen.CodeGenerator: Code generated in 25.073424 ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,904 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,908 INFO scheduler.DAGScheduler: Got job 7 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,909 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,909 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,909 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,910 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[46] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,926 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 38.8 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,928 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,929 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.105.216:33659 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,930 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,930 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[46] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,930 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,932 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4973 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:53,949 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:42537 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,238 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 306 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,239 INFO scheduler.DAGScheduler: ResultStage 10 (treeReduce at KLLRunner.scala:107) finished in 0.328 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,240 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,240 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,242 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,242 INFO scheduler.DAGScheduler: Job 7 finished: treeReduce at KLLRunner.scala:107, took 0.338082 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,528 INFO codegen.CodeGenerator: Code generated in 47.515275 ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,535 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,535 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,535 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,535 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,536 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,537 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,542 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 35.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,544 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,545 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.105.216:33659 (size: 14.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,545 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,546 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,546 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,547 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,558 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:42537 (size: 14.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,639 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 91 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,639 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,640 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.101 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,640 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,640 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,640 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,640 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,728 INFO codegen.CodeGenerator: Code generated in 51.132709 ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,741 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,743 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,743 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,743 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,743 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,744 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,748 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 21.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,751 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,752 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.105.216:33659 (size: 8.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,752 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,753 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,753 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,755 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,771 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:42537 (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,776 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,832 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,832 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,833 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.086 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,833 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,833 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,834 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.092354 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,878 INFO codegen.CodeGenerator: Code generated in 28.22048 ms\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,947 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,949 INFO scheduler.DAGScheduler: Registering RDD 62 (countByKey at ColumnProfiler.scala:592) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,949 INFO scheduler.DAGScheduler: Got job 10 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,949 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,950 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,950 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,952 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[62] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,960 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 33.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,961 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,962 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.105.216:33659 (size: 15.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,962 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,962 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[62] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,963 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,964 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:54,974 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:42537 (size: 15.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,077 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 113 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,077 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,078 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (countByKey at ColumnProfiler.scala:592) finished in 0.125 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,079 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,079 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,079 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 15)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,079 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,079 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (ShuffledRDD[63] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,081 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,083 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,083 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.105.216:33659 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,084 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,085 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[63] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,085 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,087 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,102 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:42537 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,107 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,144 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 58 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,144 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,145 INFO scheduler.DAGScheduler: ResultStage 15 (countByKey at ColumnProfiler.scala:592) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,145 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,145 INFO cluster.YarnScheduler: Killing all running tasks in stage 15: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,146 INFO scheduler.DAGScheduler: Job 10 finished: countByKey at ColumnProfiler.scala:592, took 0.197817 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,259 INFO scheduler.DAGScheduler: Registering RDD 68 (collect at AnalysisRunner.scala:326) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,260 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,260 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,260 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,260 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,261 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,265 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 86.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,267 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,268 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.105.216:33659 (size: 28.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,268 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,268 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,269 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,270 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,280 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:42537 (size: 28.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,478 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 208 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,478 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,479 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326) finished in 0.216 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,480 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,480 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,481 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,481 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,531 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,534 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,534 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,534 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,535 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,535 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,542 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 171.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,544 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 47.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,544 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.105.216:33659 (size: 47.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,544 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,545 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,545 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,546 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,557 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:42537 (size: 47.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,571 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,696 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 149 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,696 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,698 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:326) finished in 0.162 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,699 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,699 INFO cluster.YarnScheduler: Killing all running tasks in stage 18: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,700 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.166866 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,770 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,772 INFO scheduler.DAGScheduler: Registering RDD 79 (countByKey at ColumnProfiler.scala:592) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,772 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,772 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,773 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,773 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,775 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[79] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,780 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 33.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,804 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,806 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.105.216:33659 (size: 15.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,807 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,808 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[79] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,808 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,809 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4962 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,813 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:42537 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,815 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.105.216:33659 in memory (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,824 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:42537 (size: 15.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,832 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:42537 in memory (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,845 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.105.216:33659 in memory (size: 8.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,908 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.105.216:33659 in memory (size: 47.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,924 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:42537 in memory (size: 47.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,927 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 118 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,927 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,928 INFO scheduler.DAGScheduler: ShuffleMapStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.152 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,930 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,930 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,930 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 20)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,930 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,931 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,933 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,935 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,935 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.105.216:33659 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,937 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,937 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (ShuffledRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,937 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,940 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,956 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.105.216:33659 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,962 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:42537 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,963 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:42537 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,969 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.105.216:34162\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,993 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,993 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,994 INFO scheduler.DAGScheduler: ResultStage 20 (countByKey at ColumnProfiler.scala:592) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,996 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,996 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:55,996 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.226132 s\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,021 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.105.216:33659 in memory (size: 28.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,026 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:42537 in memory (size: 28.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,032 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.105.216:33659 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,034 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:42537 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,039 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.105.216:33659 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,042 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:42537 in memory (size: 15.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,055 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.105.216:33659 in memory (size: 47.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,059 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:42537 in memory (size: 47.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,069 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.105.216:33659 in memory (size: 14.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,073 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:42537 in memory (size: 14.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,080 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.105.216:33659 in memory (size: 28.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,086 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:42537 in memory (size: 28.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,094 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.105.216:33659 in memory (size: 15.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,096 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:42537 in memory (size: 15.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,116 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,164 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,164 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,172 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,188 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,235 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,235 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,242 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,254 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,304 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,308 ERROR Main: Column 'reviews.date' does not exist. Did you mean one of the following? [reviews.date, reviews.title, reviews.dateSeen, reviews.rating, reviews.text, reviews.username, dateUpdated, reviews.doRecommend, reviews.numHelpful, reviews.sourceURLs, weight, asins, dateAdded, dimension, ean, id, keys, name, brand, categories, colors, imageURLs, manufacturer, sourceURLs, upc, primaryCategories, manufacturerNumber];\u001b[0m\n",
      "\u001b[34m'Project ['reviews.date, 'reviews.dateSeen, 'reviews.doRecommend, 'reviews.numHelpful, 'reviews.rating]\u001b[0m\n",
      "\u001b[34m+- Relation [id#0,asins#1,brand#2,categories#3,colors#4,dateAdded#5,dateUpdated#6,dimension#7,ean#8,imageURLs#9,keys#10,manufacturer#11,manufacturerNumber#12,name#13,primaryCategories#14,reviews.date#15,reviews.dateSeen#16,reviews.doRecommend#17,reviews.numHelpful#18,reviews.rating#19,reviews.sourceURLs#20,reviews.text#21,reviews.title#22,reviews.username#23,... 3 more fields] csv\u001b[0m\n",
      "\u001b[34morg.apache.spark.sql.AnalysisException: Column 'reviews.date' does not exist. Did you mean one of the following? [reviews.date, reviews.title, reviews.dateSeen, reviews.rating, reviews.text, reviews.username, dateUpdated, reviews.doRecommend, reviews.numHelpful, reviews.sourceURLs, weight, asins, dateAdded, dimension, ean, id, keys, name, brand, categories, colors, imageURLs, manufacturer, sourceURLs, upc, primaryCategories, manufacturerNumber];\u001b[0m\n",
      "\u001b[34m'Project ['reviews.date, 'reviews.dateSeen, 'reviews.doRecommend, 'reviews.numHelpful, 'reviews.rating]\u001b[0m\n",
      "\u001b[34m+- Relation [id#0,asins#1,brand#2,categories#3,colors#4,dateAdded#5,dateUpdated#6,dimension#7,ean#8,imageURLs#9,keys#10,manufacturer#11,manufacturerNumber#12,name#13,primaryCategories#14,reviews.date#15,reviews.dateSeen#16,reviews.doRecommend#17,reviews.numHelpful#18,reviews.rating#19,reviews.sourceURLs#20,reviews.text#21,reviews.title#22,reviews.username#23,... 3 more fields] csv\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:199)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:192)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:192)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:192)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.immutable.Stream.foreach(Stream.scala:533)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:192)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:101)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:101)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:187)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:210)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.sql.Dataset.select(Dataset.scala:1519)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.DataAnalyzer.$anonfun$run$2(DataAnalyzer.scala:170)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.DataAnalyzer.run(DataAnalyzer.scala:163)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.DataAnalyzer.analyze(DataAnalyzer.scala:40)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.runDataQualityAnalysis(Main.scala:121)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.$anonfun$main$1(Main.scala:60)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.$anonfun$main$1$adapted(Main.scala:42)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.withSpark(Main.scala:176)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.main(Main.scala:42)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main.main(Main.scala)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[0m\n",
      "\u001b[34m#011at java.lang.reflect.Method.invoke(Method.java:498)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,314 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34mException in thread \"main\" com.amazonaws.sagemaker.dataanalyzer.exception.Error: Column 'reviews.date' does not exist. Did you mean one of the following? [reviews.date, reviews.title, reviews.dateSeen, reviews.rating, reviews.text, reviews.username, dateUpdated, reviews.doRecommend, reviews.numHelpful, reviews.sourceURLs, weight, asins, dateAdded, dimension, ean, id, keys, name, brand, categories, colors, imageURLs, manufacturer, sourceURLs, upc, primaryCategories, manufacturerNumber];\u001b[0m\n",
      "\u001b[34m'Project ['reviews.date, 'reviews.dateSeen, 'reviews.doRecommend, 'reviews.numHelpful, 'reviews.rating]\u001b[0m\n",
      "\u001b[34m+- Relation [id#0,asins#1,brand#2,categories#3,colors#4,dateAdded#5,dateUpdated#6,dimension#7,ean#8,imageURLs#9,keys#10,manufacturer#11,manufacturerNumber#12,name#13,primaryCategories#14,reviews.date#15,reviews.dateSeen#16,reviews.doRecommend#17,reviews.numHelpful#18,reviews.rating#19,reviews.sourceURLs#20,reviews.text#21,reviews.title#22,reviews.username#23,... 3 more fields] csv\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main$.main(Main.scala:109)\u001b[0m\n",
      "\u001b[34m#011at com.amazonaws.sagemaker.dataanalyzer.Main.main(Main.scala)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[34m#011at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[0m\n",
      "\u001b[34m#011at java.lang.reflect.Method.invoke(Method.java:498)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,324 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,326 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-02e930eb-c281-4ba6-8c1e-bcd655264d50\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,329 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-71c06158-3394-4801-a2ed-e71834251a99\u001b[0m\n",
      "\u001b[34m2024-07-01 15:42:56,421 - __main__ - ERROR - Exception performing analysis: Command 'bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json' returned non-zero exit status 1.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job baseline-suggestion-job-2024-07-01-15-39-42-219: Failed. Reason: AlgorithmError: Error: Errors occurred when analyzing your data. Please check CloudWatch logs for more details., exit code: 255",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m baseline_results_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformer_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/baseline_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m my_default_monitor \u001b[38;5;241m=\u001b[39m DefaultModelMonitor(\n\u001b[1;32m     14\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     15\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml.m5.xlarge\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmy_default_monitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_baseline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaseline_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDatasetFormat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_s3_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_results_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m data_quality_model_monitor \u001b[38;5;241m=\u001b[39m DefaultModelMonitor(\n\u001b[1;32m     27\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     28\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     29\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml.m5.xlarge\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m schedule \u001b[38;5;241m=\u001b[39m data_quality_model_monitor\u001b[38;5;241m.\u001b[39mcreate_monitoring_schedule(\n\u001b[1;32m     33\u001b[0m     monitor_schedule_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElectronicsDataQuality\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m     batch_transform_input\u001b[38;5;241m=\u001b[39mBatchTransformInput(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     enable_cloudwatch_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/model_monitor/model_monitoring.py:1922\u001b[0m, in \u001b[0;36mDefaultModelMonitor.suggest_baseline\u001b[0;34m(self, baseline_dataset, dataset_format, record_preprocessor_script, post_analytics_processor_script, output_s3_uri, wait, logs, job_name, monitoring_config_override)\u001b[0m\n\u001b[1;32m   1910\u001b[0m baseline_job_inputs_with_nones \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1911\u001b[0m     normalized_baseline_dataset_input,\n\u001b[1;32m   1912\u001b[0m     normalized_record_preprocessor_script_input,\n\u001b[1;32m   1913\u001b[0m     normalized_post_processor_script_input,\n\u001b[1;32m   1914\u001b[0m ]\n\u001b[1;32m   1916\u001b[0m baseline_job_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1917\u001b[0m     baseline_job_input\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m baseline_job_input \u001b[38;5;129;01min\u001b[39;00m baseline_job_inputs_with_nones\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m baseline_job_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m ]\n\u001b[0;32m-> 1922\u001b[0m \u001b[43mbaselining_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_job_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnormalized_baseline_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_baselining_job_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_baselining_job \u001b[38;5;241m=\u001b[39m BaseliningJob\u001b[38;5;241m.\u001b[39mfrom_processing_job(\n\u001b[1;32m   1932\u001b[0m     processing_job\u001b[38;5;241m=\u001b[39mbaselining_processor\u001b[38;5;241m.\u001b[39mlatest_job\n\u001b[1;32m   1933\u001b[0m )\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaselining_jobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_baselining_job)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/processing.py:277\u001b[0m, in \u001b[0;36mProcessor.run\u001b[0;34m(self, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/processing.py:1113\u001b[0m, in \u001b[0;36mProcessingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for the processing job to complete.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m    logs (bool): Whether to show the logs produced by the job (default: True).\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m-> 1113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_processing_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_processing_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5912\u001b[0m, in \u001b[0;36mSession.logs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   5909\u001b[0m             state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   5911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 5912\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5913\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   5914\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:8458\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8454\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8455\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8456\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8457\u001b[0m     )\n\u001b[0;32m-> 8458\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8459\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8460\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8461\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8462\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job baseline-suggestion-job-2024-07-01-15-39-42-219: Failed. Reason: AlgorithmError: Error: Errors occurred when analyzing your data. Please check CloudWatch logs for more details., exit code: 255"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat, BatchTransformInput, MonitoringDatasetFormat, CronExpressionGenerator\n",
    "\n",
    "role = 'arn:aws:iam::676076160400:role/LabRole'\n",
    "input_data = 's3://electronics-dataset/DatafinitiElectronicsProductData.csv'\n",
    "transformer_output = 's3://electronics-dataset/sagemaker/benchmark-model/output/'\n",
    "batch_data = 's3://electronics-dataset/sagemaker/benchmark-model/batch-data/'\n",
    "s3_report_path = 's3://electronics-dataset/sagemaker/benchmark-model/reports/'\n",
    "\n",
    "baseline_results_uri = f\"{transformer_output}/baseline_results\"\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=input_data,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "data_quality_model_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")\n",
    "\n",
    "schedule = data_quality_model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='ElectronicsDataQuality',\n",
    "    batch_transform_input=BatchTransformInput(\n",
    "        data_captured_destination_s3_uri=batch_data,\n",
    "        destination=\"/opt/ml/processing/input\",\n",
    "        dataset_format=MonitoringDatasetFormat.csv(header=False),\n",
    "    ),\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=f\"{baseline_results_uri}/statistics.json\",\n",
    "    constraints=f\"{baseline_results_uri}/constraints.json\",\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14796cd3-ec9b-4b91-9f10-64dada0a0ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, DatasetFormat, BatchTransformInput, MonitoringDatasetFormat, CronExpressionGenerator\n",
    "\n",
    "role = 'arn:aws:iam::676076160400:role/LabRole'\n",
    "input_data = 's3://electronics-dataset/DatafinitiElectronicsProductData.csv'\n",
    "transformer_output = 's3://electronics-dataset/sagemaker/benchmark-model/output/'\n",
    "batch_data = 's3://electronics-dataset/sagemaker/benchmark-model/batch-data/'\n",
    "s3_report_path = 's3://electronics-dataset/sagemaker/benchmark-model/reports/'\n",
    "\n",
    "baseline_results_uri = f\"{transformer_output}/baseline_results\"\n",
    "\n",
    "# Create DefaultModelMonitor\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")\n",
    "\n",
    "# Suggest baseline\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=input_data,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "# Create Data Quality Model Monitor\n",
    "data_quality_model_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")\n",
    "\n",
    "# Schedule Data Quality Monitoring\n",
    "schedule = data_quality_model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='ElectronicsDataQuality',\n",
    "    batch_transform_input=BatchTransformInput(\n",
    "        data_captured_destination_s3_uri=batch_data,\n",
    "        destination=\"/opt/ml/processing/input\",\n",
    "        dataset_format=MonitoringDatasetFormat.csv(header=False),\n",
    "    ),\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=f\"{baseline_results_uri}/statistics.json\",\n",
    "    constraints=f\"{baseline_results_uri}/constraints.json\",\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce7fbd-43e6-40bb-92b8-fcd8e2ee5151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
